{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9805df82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "names=('Santhosh','Sunny','Bunny','Sreeja')\n",
    "named_array=np.array(names)\n",
    "named_array.shape\n",
    "arr0=np.array(1)\n",
    "arr1=np.array((1,2))\n",
    "arr2=np.array(((1,2),(3,4)))\n",
    "arr2.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d5cc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3d array\n",
    "import numpy as np\n",
    "arr=np.array([[[1,2,3],[4,5,6]],[[7,8,9],[10,11,12]]])\n",
    "arr.ndim\n",
    "#accessing elements in the 3d array\n",
    "arr[0,1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "c6817e00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    101\n",
       "4    105\n",
       "Name: Id, dtype: int64"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#result_df = df.groupby('Id').filter(lambda group: set(group['Subject']) == {'Sql'})\n",
    "import pandas as pd\n",
    "data=(\n",
    "    (101,'Sql'),(102,'Sql'),(102,'Python'),(103,'Sql'),(103,'Excel'),(104,'Python'),(104,'Excel'),(105,'Sql'),\n",
    "    (106,'Python')\n",
    ")\n",
    "fil='Sql'\n",
    "df=pd.DataFrame(data, columns=('Id','Subject'))\n",
    "df.set_index('Id', inplace=True)\n",
    "result_df = df.groupby('Id')['Subject'].agg(', '.join).reset_index()\n",
    "result_df['Id'][result_df['Subject'].str.fullmatch(fil)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a4879e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define data properly as a list of dictionaries\n",
    "data = [\n",
    "    {\"name\": \"Gilbert\", \"wins\": [[\"straight\", \"7♣\"], [\"one pair\", \"10♥\"]]},\n",
    "    {\"name\": \"Alexa\", \"wins\": [[\"two pair\", \"4♠\"], [\"two pair\", \"9♠\"]]},\n",
    "    {\"name\": \"May\", \"wins\": []},\n",
    "    {\"name\": \"Deloise\", \"wins\": [[\"three of a kind\", \"5♣\"]]}\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df_exploded = df.explode('wins').explode('wins')\n",
    "print(df_exploded.reindex())\n",
    "#df_grouped = df_exploded.groupby(\"name\").agg(new_field=(\"wins\", lambda x: ', '.join(map(str, x))))\n",
    "df_grouped = df_exploded.groupby(\"name\").agg(Collections=(\"wins\", lambda x: ', '.join(map(str,x))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "e39fecb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Subject</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>Sql</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102</td>\n",
       "      <td>Sql, Python</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103</td>\n",
       "      <td>Sql, Excel</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>104</td>\n",
       "      <td>Python, Excel</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>105</td>\n",
       "      <td>Sql</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>106</td>\n",
       "      <td>Python</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Id        Subject  Rating\n",
       "0  101            Sql     4.0\n",
       "1  102    Sql, Python     9.0\n",
       "2  103     Sql, Excel     8.5\n",
       "3  104  Python, Excel     8.5\n",
       "4  105            Sql     3.5\n",
       "5  106         Python     5.0"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data=(\n",
    "    (101,'Sql',4),(102,'Sql',4),(102,'Python',5),(103,'Sql',3.5),(103,'Excel',5),(104,'Python',4.5)\n",
    "    ,(104,'Excel',4),(105,'Sql',3.5), (106,'Python',5)\n",
    ")\n",
    "df=pd.DataFrame(data, columns=('Id','Subject','Rating'))\n",
    "#Giving alieas names to the aggregated columns\n",
    "grouped_df = df.groupby(by='Id').agg({'Subject': ', '.join,'Rating': 'sum'}).reset_index()\n",
    "grouped_df\n",
    "#grouped_df[grouped_df['Subject'].str.fullmatch('Sql')]['Id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009c9d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "arr=np.array([[1,2,3],[4,5,6]])\n",
    "a=np.array([1,2,3,4,5])\n",
    "b=[1,2,3,4,5]\n",
    "b[1:4]=[0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf05487",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = {'Category': ['A', 'B', 'A', 'B'],\n",
    "        'Type': ['X', 'X', 'Y', 'Y'],\n",
    "        'Value1': [10, 20, 30, 40],\n",
    "        'Value2': [5, 15, 25, 35]}\n",
    "df = pd.DataFrame(data)\n",
    "grouped_df=df.groupby(by=['Category','Type'])\n",
    "final_df=grouped_df.agg({'Value1': 'sum','Value2': 'max'}).reindex()\n",
    "final_df[final_df['Value2']>10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdd803b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = {'Category': ['A', 'A', 'B', 'B', 'C', 'C'],\n",
    "        'Value': [100, 50, 30, 20, 70, 90]}\n",
    "df = pd.DataFrame(data)\n",
    "#Filtering groups where the sum of 'Value' is greater than 100\n",
    "filtered = df.groupby('Category').filter(lambda x: x['Value'].sum() > 100)\n",
    "print(filtered)\n",
    "grp_df=df.groupby('Category').agg(Summation=('Value','sum'))\n",
    "grp_df[grp_df['Summation']>100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684eebfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = {\n",
    "    'Store': ['A', 'A', 'B', 'B', 'C', 'C'],\n",
    "    'Product': ['X', 'Y', 'X', 'Y', 'X', 'Y'],\n",
    "    'Sales': [200, 150, 300, 250, 400, 100],\n",
    "    'Quantity': [3, 5, 2, 4, 6, 3]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "agg_df_reset=df.groupby('Store').agg(\n",
    "    Total_products=('Product','count'),\n",
    "    Total_sales=('Sales','sum'),\n",
    "    Total_quantity=('Quantity','sum')\n",
    ")\n",
    "agg_df_reset[agg_df_reset['Total_sales']>300]\n",
    "#filtered_stores = agg_df_reset[agg_df_reset['Total_sales'] > 350].index\n",
    "#print(filtered_stores)\n",
    "\n",
    "#filt=agg_df_reset.reset_index()\n",
    "#filt\n",
    "#filt['Store'][filt['Total_sales']>350]\n",
    "#fil=agg_df_reset[agg_df_reset['Total_sales']>350]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ea186c",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr=[\"20250119 20250120\"]\n",
    "ar1=arr[0].split(' ')\n",
    "ar1[0],ar1[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00cab19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "d1=(\n",
    "    {'Id':1,\"Name\":'Santhosh','Dept_No':1},\n",
    "    {'Id':2,\"Name\":'Sunny','Dept_No':2},\n",
    "    {'Id':3,\"Name\":'Bunny','Dept_No':2},\n",
    "    {'Id':4,\"Name\":'Sumhith','Dept_No':3},\n",
    "     {'Id':4,\"Name\":'Hero','Dept_No':4}\n",
    ")\n",
    "d2=(\n",
    "    {'Dept_No':1,'Place':'Hyderabad'},\n",
    "    {'Dept_No':2,'Place':'Kamareddy'},\n",
    "    {'Dept_No':3,'Place':'Karimnagar'},\n",
    "    {'Dept_No':4,'Place':'Warangal'}\n",
    ")\n",
    "df1=pd.DataFrame(d1)\n",
    "df2=pd.DataFrame(d2)\n",
    "\n",
    "f=df1.merge(df2,how='inner',on='Dept_No')\n",
    "f[f['Dept_No']==2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1ac357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# First DataFrame with a common column \"Salary\"\n",
    "df1 = pd.DataFrame({\n",
    "    'Id': [1, 2, 3],\n",
    "    'Name': ['Alice', 'Bob', 'Charlie'],\n",
    "    'Dept_No': [1, 2, 3],\n",
    "    'Salary': [5000, 6000, 7000]  # Common field\n",
    "})\n",
    "# Second DataFrame with same \"Salary\" column\n",
    "df2 = pd.DataFrame({\n",
    "    'Dept_No': [1, 2, 3],\n",
    "    'Place': ['New York', 'Chicago', 'Los Angeles'],\n",
    "   # 'Salary': [5500, 6200, 7100]  # Common field\n",
    "})\n",
    "# Merge DataFrames\n",
    "merged_df = df1.merge(df2, how='inner', on='Dept_No')\n",
    "print(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d11fadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "people = pd.DataFrame([\n",
    "    {\"deptId\": 1, \"age\": 40, \"name\": \"Hyukjin Kwon\", \"gender\": \"M\", \"salary\": 50},\n",
    "    {\"deptId\": 1, \"age\": 50, \"name\": \"Takuya Ueshin\", \"gender\": \"M\", \"salary\": 100},\n",
    "    {\"deptId\": 2, \"age\": 60, \"name\": \"Xinrong Meng\", \"gender\": \"F\", \"salary\": 150},\n",
    "    {\"deptId\": 3, \"age\": 20, \"name\": \"Haejoon Lee\", \"gender\": \"M\", \"salary\": 200}\n",
    "])\n",
    "age_filter=[40,50]\n",
    "people[people['age'].between(*age_filter)]['name']\n",
    "\n",
    "dept_grp_df=people.groupby(by='deptId').agg(Total_Sal=('salary','sum'))\n",
    "dept_no_df=dept_grp_df[dept_grp_df['Total_Sal'].between(155,200)].reset_index()\n",
    "dept_no=dept_no_df['deptId']\n",
    "dept_no.index \n",
    "#dept_no.to_list\n",
    "people[people['deptId'].isin(dept_no)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "8004280b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Names Genders  Avg_Salary  Max_Age\n",
      "deptId                                      \n",
      "1       PySpark       M        75.0       50\n",
      "2            ML       F       150.0       60\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Creating DataFrames\n",
    "people = pd.DataFrame([\n",
    "    {\"deptId\": 1, \"age\": 40, \"name\": \"Hyukjin Kwon\", \"gender\": \"M\", \"salary\": 50},\n",
    "    {\"deptId\": 1, \"age\": 50, \"name\": \"Takuya Ueshin\", \"gender\": \"M\", \"salary\": 100},\n",
    "    {\"deptId\": 2, \"age\": 60, \"name\": \"Xinrong Meng\", \"gender\": \"F\", \"salary\": 150},\n",
    "    {\"deptId\": 3, \"age\": 20, \"name\": \"Haejoon Lee\", \"gender\": \"M\", \"salary\": 200}\n",
    "])\n",
    "department = pd.DataFrame([\n",
    "    {\"id\": 1, \"deptName\": \"PySpark\"},  # Rename 'name' to 'deptName' to avoid conflicts\n",
    "    {\"id\": 2, \"deptName\": \"ML\"},\n",
    "    {\"id\": 3, \"deptName\": \"Spark SQL\"}\n",
    "])\n",
    "# Merge the dataframes\n",
    "people_dept_join = people.merge(department, left_on=\"deptId\", right_on=\"id\", how=\"inner\")\n",
    "# Filter records where age > 30\n",
    "age_fil_df = people_dept_join[people_dept_join['age'] > 30]\n",
    "# Group by deptId while keeping name & gender as lists\n",
    "grp_df = age_fil_df.groupby(by='deptId').agg(\n",
    "    Names=('deptName','max' ),\n",
    "    Genders=('gender', 'max'),\n",
    "    Avg_Salary=('salary', 'mean'),\n",
    "    Max_Age=('age', 'max')\n",
    ")\n",
    "print(grp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a6b069",
   "metadata": {},
   "outputs": [],
   "source": [
    "import  pandas as pd \n",
    "file=r'C:\\Users\\jangasa\\OneDrive - Republic Services\\Desktop\\Jan25 Prod Commissions.csv'\n",
    "df=pd.read_csv(file,encoding='windows-1252',low_memory=False).reindex()\n",
    "df.fillna(0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0e7bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"StructTypeExample\").getOrCreate()\n",
    "df=spark.read.csv(path=r'C:\\Users\\jangasa\\OneDrive - Republic Services\\Desktop\\My_Folder\\Python\\nyc_temperature.csv',encoding='latin1',header=True)\n",
    "df.show(100)\n",
    "#display(df)\n",
    "#df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940bb54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"StructTypeExample\").getOrCreate()\n",
    "df=spark.read.format(source='csv').options(headers=True).load(path=r'C:\\Users\\jangasa\\OneDrive - Republic Services\\Desktop\\My_Folder\\Python\\nyc_temperature.csv')\n",
    "df.show()\n",
    "#help(spark.read.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd870c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"StructTypeExample\").getOrCreate()\n",
    "csv_file_list = []\n",
    "directory_path = r'C:\\Users\\jangasa\\OneDrive - Republic Services\\Desktop'\n",
    "entries = os.scandir(directory_path)\n",
    "for entry in entries:\n",
    "    if entry.isfile() and entry.name.endswith('.csv'):\n",
    "        csv_file_list.append(entry.path)  # Store full file path\n",
    "# Ensure csv_file_list is not empty before loading\n",
    "if csv_file_list:\n",
    "    df = spark.read.csv(path=csv_file_list, header=True, inferSchema=True)\n",
    "    df.show()\n",
    "else:\n",
    "    print(\"No CSV files found in the specified directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fa35dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing the df into csv files\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"WritingtoCSV\").getOrCreate()\n",
    "df=spark.read.format(source='csv').option('header',True).load(path=r'C:\\Users\\jangasa\\OneDrive - Republic Services\\Desktop\\My_Folder\\Python\\nyc_temperature.csv')\n",
    "#df=spark.read.format(source='csv').options(header=True).load(path=r'C:\\Users\\jangasa\\OneDrive - Republic Services\\Desktop\\My_Folder\\Python\\nyc_temperature.csv')\n",
    "#help(df.write.csv)\n",
    "df.coalesce(1).write.option(\"header\", True).csv(r'C:\\Users\\jangasa\\OneDrive - Republic Services\\Desktop\\temperature\\temp.csv', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f361bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading jsonfile into dataframe\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, explode\n",
    "spark = SparkSession.builder.appName(\"WritingtoCSV\").getOrCreate()\n",
    "df=spark.read.json(path=r'C:\\Users\\jangasa\\OneDrive - Republic Services\\Desktop\\My_Folder\\Python\\Json2.json',multiLine=True)\n",
    "#df.show(truncate=False)\n",
    "# Create DataFrame\n",
    "#df = spark.read.json(spark.sparkContext.parallelize(data))\n",
    "\n",
    "# Flatten JSON structure\n",
    "df_flattened = df.select(\n",
    "    col(\"bow\"),\n",
    "    col(\"darkness\"),\n",
    "    col(\"proper\"),\n",
    "    col(\"worse\"),\n",
    "    col(\"tree\"),\n",
    "    col(\"see.star\").alias(\"star\"),\n",
    "    col(\"see.chief\").alias(\"chief\"),\n",
    "    col(\"see.sugar\").alias(\"sugar\"),\n",
    "    col(\"see.straight\").alias(\"straight\"),\n",
    "    col(\"see.cake\").alias(\"cake\"),\n",
    "    explode(col(\"see.these\")).alias(\"these_values\")  # Flatten list inside \"see\"\n",
    ")\n",
    "df_flattened.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7154933d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading jsonfile into dataframe\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, explode\n",
    "#spark = SparkSession.builder.appName(\"WritingtoCSV\").getOrCreate()\n",
    "df=spark.read.json(path=r'C:\\Users\\jangasa\\OneDrive - Republic Services\\Desktop\\My_Folder\\Python\\Json2.json',multiLine=True)\n",
    "print(df.schema.fields)\n",
    "def flattend(df):\n",
    "    for field in df.schema.fields:\n",
    "        if field.dataType.simpleString().startswith(\"struct\"):\n",
    "            new_fileds=[col(f\"{field.name}.{subfield.name}\")\\\n",
    "                        .alias(f\"{field.name}_{subfield.name}\") for subfield in field.dataType]\n",
    "            df=df.select('*',*new_fileds).drop(field.name)\n",
    "        elif field.dataType.simpleString().startswith(\"array\"): # Check for lists\n",
    "            df = df.withColumn(field.name, explode(col(field.name)))\n",
    "    return df\n",
    "df=flattend(df).withColumn('see_these',explode(col('see_these')))\n",
    "#df.show()\n",
    "\n",
    "#df.withColumn('bow', col('bow').cast(\"int\")).show()\n",
    "#Writing into json\n",
    "#df.write.json(path=r'C:\\Users\\jangasa\\OneDrive - Republic Services\\Desktop\\My_Folder\\Python\\abc\\abc.json', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932e9496",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"parquet Example\").getOrCreate()\n",
    "df=spark.read.parquet(r'C:\\Users\\jangasa\\OneDrive - Republic Services\\Desktop\\My_Folder\\Python\\titanic.parquet')\n",
    "#df.schema.fields\n",
    "df1=df.toPandas().head(5)\n",
    "#df.show(vertical=True,n=2, truncate=0)\n",
    "#df.select('passengerId','Survived').filter(df.PassengerId<15).show()\n",
    "#help(spark.read.parquet)\n",
    "df.schema.fields\n",
    "#help(df.write.parquet)\n",
    "#help(df.withColumn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d228c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "spark=SparkSession.builder.appName('Emp_Dept').getOrCreate()\n",
    "df=spark.read.csv(r'C:\\Users\\jangasa\\OneDrive - Republic Services\\Desktop\\My_Folder\\Python\\Employee.csv', header=True)\n",
    "#Additing deptname column based on the dept no\n",
    "df=df.withColumn('Dept_Name',when(col('Dept_No')==10,'London')\n",
    "                            .when(col('Dept_No')==20,'NewYork')\n",
    "                            .otherwise('India'))\n",
    "#df.printSchema()\n",
    "df.schema.fields\n",
    "#df.dtypes\n",
    "#Renaming tghe 'Dept_Name' to Location\n",
    "df=df.withColumnRenamed('Dept_Name','Location')\n",
    "#df.show()\n",
    "#help(df.withColumnRenamed)\n",
    "help(ArrayType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be5e86a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id      Name     Subjects\n",
      "0   1  Santhosh          sql\n",
      "0   1  Santhosh    snowflake\n",
      "0   1  Santhosh         IICS\n",
      "1   2     Sunny         Java\n",
      "1   2     Sunny  java script\n",
      "1   2     Sunny       python\n"
     ]
    }
   ],
   "source": [
    "import  pandas as pd\n",
    "data=[(1,'Santhosh',('sql','snowflake','IICS')),(2,'Sunny',('Java','java script','python'))]\n",
    "columns=['Id','Name','Subjects']\n",
    "df=pd.DataFrame(data=data,columns=columns)\n",
    "df_exploded = df.explode('Subjects')\n",
    "print(df_exploded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307f65b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = [\n",
    "    (1, 'Santhosh', {\"Subjects\": ('sql', 'snowflake', 'IICS')}),\n",
    "    (2, 'Sunny', {\"Subjects\": ('Java', 'java script', 'python')})]\n",
    "columns = ['Id', 'Name', 'Subjects']\n",
    "df = pd.DataFrame(data=data, columns=columns)\n",
    "df['Subjects'] = df['Subjects'].apply(lambda x: x.get(\"Subjects\", []))\n",
    "df = df.explode('Subjects')\n",
    "print(df)\n",
    "#grp_df = df.groupby(['Id', 'Name']).agg(**{'list_of_Sub': ('Subjects', ','.join)})\n",
    "#for dictionaries\n",
    "grp_df = df.groupby(['Id', 'Name']).agg({'Subjects': ', '.join}).rename(columns={'Subjects': 'list_of_Sub'})\n",
    "print(grp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34053be2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
